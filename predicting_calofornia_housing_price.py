# -*- coding: utf-8 -*-
"""Predicting Calofornia Housing Price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f6ccr7BKHY_s6hZt1qSUPjGSUp3oTzZ6

# **Importing housing data and extracting the csv file **
"""

import os
import tarfile
from six.moves import urllib

Download_root='https://raw.githubusercontent.com/ageron/handson-ml/master/'
Housing_path=os.path.join('datasets','housing')
Housing_url=Download_root+'datasets/housing/housing.tgz'

def fetch_housing_data(housing_url=Housing_url,housing_path=Housing_path):
  if not os.path.isdir(housing_path):
    os.makedirs(housing_path)

  tgz_path=os.path.join(housing_path,'housing.tgz')
  urllib.request.urlretrieve(housing_url,tgz_path)
  housing_tgz=tarfile.open(tgz_path) 
  housing_tgz.extractall(path=housing_path) 
  housing_tgz.close()

fetch_housing_data()

"""# ***Loadind data Using Pandas***"""

import pandas as pd 
import numpy as np

def load_housing_data(housing_path= Housing_path):
  csv_path=os.path.join(housing_path,'housing.csv')
  return pd.read_csv(csv_path)

housing=load_housing_data()
housing.head()

"""# **Information about housing data**"""

housing.info()
housing.describe()

"""From the above information of the data , we can say that this data contains 10 columns , 20640 rows or instances. 1 categorical column name= 'ocean_proximity'
and there are some missing values in total_bedroom clumns. We have to deal with it !

# **Categorize the data according to Ocean_proximity**
"""

housing['ocean_proximity'].value_counts()

"""# ***Visualization of data***
Histogram of each attribute. which you can create by calling hist() function.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

housing.hist(bins=50,figsize=(20,15))
plt.show()

"""Considering that **"medium income"** is the most important feature to predict the pricing of the house, we will create our test and training set according to the prrcentage of medium income in whole data set. Meaning the percentage will be also same in our test data set as our whole data.
From the above histogram it is seen that medium income is significant between **1.5 to 6 in value in the x axis.** So, we will create a categorical value between this range using pd.cut() fuction. Each range will have a specific label like- **0 - 1.5** will be under **label 1**, and **1.5-3** will be under **label 2** and so on.

***Ploting the histogram of the labeled data also for easy visualization of our data***
"""

housing['income_cat']=pd.cut(housing['median_income'],bins=[0,1.5,3,4.5,6,np.inf],labels=[1,2,3,4,5])
housing['income_cat'].hist()

"""# **Splitting test set using Stratifiedshufflesplit from SK learn**

To keep the percentage of medium income same in whole datset and test data,we are using stratified method rather than normal train test split method.

You can measure the similarity of class percentage between two data according to income category.
"""

from sklearn.model_selection import StratifiedShuffleSplit

split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)
for train_index,test_index in split.split(housing,housing['income_cat']):
  strat_train_set=housing.loc[train_index]
  strat_test_set=housing.loc[test_index]

print("test set",strat_test_set['income_cat'].value_counts()/len(strat_test_set) )
housing['income_cat'].value_counts()/len(housing)

#As spliting is done so removing the income cat attribute from the data to get the original data back

for set_ in (strat_train_set,strat_test_set):
  set_.drop('income_cat',axis=1,inplace=True)

"""# **Visualizing geographical data as there are a attributes longitde and latitude**"""

housing=strat_train_set.copy()
housing.plot(kind="scatter",x="longitude",y='latitude')
print(housing.head())

housing.plot(kind="scatter",x="longitude",y='latitude' ,alpha=0.1) 

#alpha make it possibile to visualize the places where the data density is high

#radius of each circle represent the district population (s)
#the color represnt the price (c)

housing.plot(kind="scatter",x="longitude",y='latitude' ,alpha=0.4,s=housing['population']/100,label='populaion',
             figsize=(10,7),c=housing['median_house_value'],cmap=plt.get_cmap('jet'),colorbar=True)
plt.legend()

"""# **Correlation between the features**"""

corr_matrix=housing.corr()
corr_matrix['median_house_value'].sort_values(ascending=False)

"""**Correlation** **using pandas scatter_matrix**"""

from pandas.plotting import scatter_matrix

attributes=['median_house_value','median_income','total_rooms','housing_median_age']
scatter_matrix(housing[attributes],figsize=(12,8))

"""The main diagonal from top left to bottom right would be a straight line if pandas plot them against itself, which would not be very useful that is why histogram is shown.


**The most promising attribute to predict median house value seems like median income. so just lets jumpt to its plot.**
"""

housing.plot(kind="scatter",x="median_income",y='median_house_value' ,alpha=0.1)

"""# **Experimenting with attributes**"""

housing['rooms_per_household']=housing['total_rooms']/housing['households']
housing['bedroom_per_room']=housing['total_bedrooms']/housing['total_rooms']
housing['population_per_household']=housing['population']/housing['households']

corr_matrix=housing.corr()
corr_matrix['median_house_value'].sort_values(ascending=False)

"""# **ADDing some new features which is experiemnted in the previous cell, using custom estimator**"""

#using custom transformer

from sklearn.base import BaseEstimator, TransformerMixin

room_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6

class combinedattributesadder(BaseEstimator,TransformerMixin):
    def __init__(self,add_bedroomsper_room=True):
     self.add_bedroomsper_room=add_bedroomsper_room

    def fit(self,X,y=None):
     return self

    def transform(self,X,y=None):
      rooms_per_household=X[:,room_ix]/X[:,households_ix]
      population_per_household=X[:,population_ix]/X[:,households_ix]
      if  self.add_bedroomsper_room:
        bedrooms_per_room=X[:,bedrooms_ix]/X[:,room_ix]
        return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]
      else:
        return np.c_[X,rooms_per_household,population_per_household]




attr_adder=combinedattributesadder(add_bedroomsper_room=False)
housing_extra_attribes=attr_adder.transform(housing.values)

"""# **Separating features and labels**"""

housing=strat_train_set.drop("median_house_value",axis=1)
housing_labels=strat_train_set["median_house_value"].copy()

"""## **Filling missing values**

filling them with the median value of that particular attribute using simpleimputer from sk learn.

**Droping the ocean_proximity column as it is a categorical value and its median cant be calculated.**
"""

from sklearn.impute import SimpleImputer

imputer=SimpleImputer(strategy='median')

housing_num=housing.drop('ocean_proximity', axis=1)
housing_cat=housing['ocean_proximity']
imputer.fit(housing_num)

print(imputer.statistics_) # this variable contain all the median values of each column

X=imputer.transform(housing_num)

housing_tr=pd.DataFrame(X,columns=housing_num.columns)

housing_tr

"""## Handelling categorical data "Ocean_proximity" using oneHotEncoder"""

from sklearn.preprocessing import OneHotEncoder
cat_encoder=OneHotEncoder()

"""# **Transformation Pipelines**

**Filling missing values** - using imputer

**Adding new feature** - using custom estimator

**Feature scaling** - using standard scaler from sklearn




> Indented block 

Doing these transformations all together by calling pipeline function from sklearn
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline=Pipeline([
                       ('imputer',SimpleImputer(strategy='median')),
                       ('attr_adder',combinedattributesadder()),
                       ('std_scaler',StandardScaler())
                       ])

housing_num_tr=num_pipeline.fit_transform(housing_num)

"""# **Columns Transformer**

We have handled the categorical and numerical values separately but it can be handled together using sklearn function
"""

from sklearn.compose import ColumnTransformer
num_attribs=list(housing_num)
cat_attribs=['ocean_proximity']

full_pipeline=ColumnTransformer([
                                 ('num',num_pipeline,num_attribs),
                                 ('cat',OneHotEncoder(),cat_attribs),
])
housing_prepared=full_pipeline.fit_transform(housing)

"""# **Linear Regression model**"""

from sklearn.linear_model import LinearRegression
lin_reg=LinearRegression()
lin_reg.fit(housing_prepared,housing_labels)

from sklearn.metrics import mean_squared_error
housing_predictions=lin_reg.predict(housing_prepared)
lin_mse=mean_squared_error(housing_labels,housing_predictions)
lin_rmse=np.sqrt(lin_mse)
lin_rmse

"""# **Decision Tree Regression**"""

from sklearn.tree import DecisionTreeRegressor
tree_reg=DecisionTreeRegressor()
tree_reg.fit(housing_prepared,housing_labels)
housing_predictions=tree_reg.predict(housing_prepared)
tree_mse=mean_squared_error(housing_labels,housing_predictions)
tree_rmse=np.sqrt(tree_mse)
tree_rmse

"""# **Decision tree regression with cross validation**"""

from sklearn.model_selection import cross_val_score

scores=cross_val_score(tree_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)

tree_rmse_scores=np.sqrt(-scores)

def display_scores(scores):
  print("scores",scores)
  print("mean",scores.mean())
  print('standard deviation',scores.std())

display_scores(tree_rmse_scores)

"""# **Linear regression with Cross validation**"""

scores=cross_val_score(lin_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)

lin_rmse_scores=np.sqrt(-scores)

def display_scores(scores):
  print("scores",scores)
  print("mean",scores.mean())
  print('standard deviation',scores.std())

display_scores(lin_rmse_scores)

"""# **Random forest regression**"""

from sklearn.ensemble import RandomForestRegressor

forest_reg=RandomForestRegressor()
forest_reg.fit(housing_prepared,housing_labels)

housing_predictions=forest_reg.predict(housing_prepared)
forest_mse=mean_squared_error(housing_labels,housing_predictions)
forest_rmse=np.sqrt(forest_mse)
forest_rmse

"""# **Random forest regression with Cross validation**"""

forest_scores=cross_val_score(forest_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)

forest_rmse_scores=np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)

"""# **Parameter Tunning with GridSearchCv**"""

from sklearn.model_selection import GridSearchCV

param_grid=[
            {'n_estimators':[3,10,30],'max_features':[2,4,6,8]},
            {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}
]
forest_reg=RandomForestRegressor()
grid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring='neg_mean_squared_error',return_train_score=True)
grid_search.fit(housing_prepared,housing_labels)
grid_search.best_params_

grid_search.best_estimator_

cvres=grid_search.cv_results_
for mean_score,params in zip(cvres['mean_test_score'],cvres['params']):
  print(np.sqrt(-mean_score),params)

"""# **Final evaluation of the model**"""

final_model=grid_search.best_estimator_

X_test=strat_test_set.drop('median_house_value',axis=1)
y_test=strat_test_set['median_house_value'].copy()

X_test_pepared=full_pipeline.transform(X_test)

final_prediction=final_model.predict(X_test_pepared)
final_mse=mean_squared_error(y_test,final_prediction)

final_rmse=np.sqrt(final_mse)
final_rmse